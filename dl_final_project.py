# -*- coding: utf-8 -*-
"""DL_Final_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xCMkmFvv50KyNDcl5w0Z1zrJd7hUVTlP

**Smart Credit Card Approval System**

Import necessary libraries
"""

# Import necessary libraries for data manipulation, model training, and evaluation
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import classification_report, confusion_matrix

"""Load dataset"""

# Load dataset
data = pd.read_csv("/content/clean_dataset.csv")

"""Explore The data

"""

print(data.head())
print(data.info())
print(data.describe())

"""Preprocessing"""

# Fill missing values with median (only applicable to numeric columns)
data.fillna(data.median(numeric_only=True), inplace=True)

# Encode categorical variables
for column in data.select_dtypes(include=['object']).columns:
    data[column] = LabelEncoder().fit_transform(data[column])

# Separate features and target
# Assuming the target column is named 'Approval_Status'
X = data.drop("Approved", axis=1)  # Modify if your target column has a different name
y = data["Approved"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Build the Model
model = Sequential([
    Dense(64, input_dim=X_train.shape[1], activation='relu'),
    Dropout(0.2),  # Prevent overfitting
    Dense(32, activation='relu'),
    Dropout(0.2),
    Dense(1, activation='sigmoid')  # Sigmoid for binary classification
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Add early stopping to prevent overfitting
early_stop = EarlyStopping(monitor='val_loss', patience=10)

# Train the model
history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, batch_size=32, callbacks=[early_stop])

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy:.2f}")

# Make predictions
y_pred = (model.predict(X_test) > 0.5).astype("int32")

# Add predictions to a DataFrame for better readability
predictions_df = pd.DataFrame({
    'Actual': y_test,
    'Predicted': y_pred.flatten()
})
# Map numerical predictions to "Approved" or "Not Approved"
predictions_df['Prediction_Text'] = predictions_df['Predicted'].map({1: "Approved", 0: "Not Approved"})

# Display the predictions
print(predictions_df)

# Generate a classification report
class_report = classification_report(y_test, y_pred)
print("Classification Report:")
print(class_report)
